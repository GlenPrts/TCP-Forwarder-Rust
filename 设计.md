好的，这是基于我们所有讨论的、一份完整的、工业级的TCP转发器设计文档。

---

## **高性能TCP转发器 - 系统设计文档 (V1.0)**

### 1. 概述

#### 1.1. 项目目标

本文档旨在设计一个高性能、高可用、可观测性强的工业级TCP转发器。该转发器旨在作为关键服务（如数据库、缓存集群等）的可靠接入层，提供智能的负载均衡、故障切换和连接管理能力。

#### 1.2.核心特性

*   **动态后端发现**: 支持从文件、HTTP API、DNS SRV记录等多种来源动态发现和更新后端服务列表。
*   **智能健康探测**: 内置多种探测策略（周期性爆发、连续分批），对后端服务进行持续的健康检查，并支持并发控制。
*   **高级评分与熔断**: 基于延迟、成功率等指标对后端进行量化评分，并集成熔断器机制，实现快速故障隔离与恢复。
*   **灵活的负载均衡**: 提供两阶段选择器，支持丰富的候选人过滤（Top-N/Top-%）和选择策略（P2C、IP哈希、加权随机等）。
*   **高性能连接池**: 维护一个可自动伸缩的暖连接池，预先建立TCP连接，显著降低业务请求的连接延迟。
*   **企业级可观测性**: 支持结构化（JSON/Text）日志输出到文件或控制台，便于监控、告警和故障排查。
*   **高度可配置**: 所有核心行为均可通过单一YAML配置文件进行调整，无需重新编译。

### 2. 设计原则

*   **单一职责原则 (SRP)**: 每个组件只做一件事，并把它做好。
*   **开闭原则 (OCP)**: 系统对扩展开放，对修改关闭。通过接口和策略模式，可以轻松添加新的后端发现方式、选择策略等，而无需修改现有核心代码。
*   **依赖倒置原则 (DIP)**: 组件之间通过接口（抽象）交互，而非具体实现，实现松耦合。
*   **可配置性**: 核心逻辑与配置分离，运行时行为由外部配置决定。
*   **鲁棒性**: 在设计中充分考虑失败场景，如后端不可用、配置错误等，并具备优雅处理和恢复的能力。

### 3. 高层架构

系统由一系列高内聚、低耦合的组件构成，通过明确定义的接口协同工作。

#### 3.1. 架构图

```
+-----------------------------------------------------------------------------------+
|                                 TCP Forwarder Process                             |
|                                                                                   |
|  +--------------+       +-------------------+       +-------------------------+   |
|  | Client Conn  | ----> |   ProxyServer     | ----> |   ConnectionManager     |   |
|  +--------------+       +-------------------+       +-------------------------+   |
|                                                          |           ^            |
|       +--------------------------------------------------+-----------+--------+   |
|       |              |                       |           |           |        |   |
|       v              v                       v           v           |        |   |
| +-----------+  +-------------+       +---------------+ +----------+  |        |   |
| | ISelector |  |IHealthProber| <---> | IScoringSystem| | IWarmPool|  |        |   |
| +-----------+  +-------------+       | (  Circuit    | +----------+  |        |   |
|       ^                              |    Breaker  ) |       ^       |        |   |
|       |                              +---------------+       |       |        |   |
|       +------------------------------------+-----------------+       |        |   |
|                                            |                         |        |   |
|                                            v                         v        |   |
|                                +--------------------------+          |        |   |
|                                |   TCP Connection Logic   |----------+        |   |
|                                +--------------------------+                   |   |
|                                            ^                                  |   |
|                                            |                                  |   |
|  +------------------------+      +-------------------------+                  |   |
|  |   IBackendProvider     |----> | (Updates Backend List)  |<-----------------+   |
|  | (File, HTTP, DNS_SRV)  |      +-------------------------+                  |   |
|  +------------------------+                                                   |   |
|                                                                               |   |
|  +----------------------------------------------------------------------------+   |
|  |                            Logger & Config                                 |   |
|  +----------------------------------------------------------------------------+   |
|                                                                                   |
+-----------------------------------------------------------------------------------+
```

#### 3.2. 核心组件

*   **ProxyServer**: 系统的入口，负责监听和接受客户端连接。
*   **ConnectionManager**: 系统的核心协调器，处理客户端请求，并协调其他组件以获取一个健康的后端连接。
*   **IBackendProvider**: 后端服务提供者，负责从外部源获取后端IP列表。
*   **IHealthProber**: 健康探测器，负责持续地对后端服务进行健康检查。
*   **IScoringSystem**: 评分系统，维护每个后端的健康分数、状态和熔断器。
*   **ISelector**: 选择器，根据评分和策略，从可用后端中选择一个。
*   **IWarmPool**: 暖连接池，存储预先建立好的TCP连接。
*   **Logger & Config**: 贯穿所有组件的基础设施，提供日志记录和配置读取能力。

### 4. 组件详细设计

#### 4.1. ProxyServer
*   **职责**:
    1.  监听配置的 `listenAddr`。
    2.  接受客户端TCP连接 (`clientConn`)。
    3.  为每个 `clientConn` 派生一个独立的执行单元（如goroutine）。
    4.  在该执行单元中，调用 `ConnectionManager` 获取后端连接。
    5.  建立客户端与后端的数据转发通道。
    6.  连接结束后，确保关闭客户端和后端连接。

#### 4.2. ConnectionManager
*   **职责**:
    1.  **处理连接请求**: 提供 `GetBackendConnection()` 方法，该方法从 `IWarmPool` 中借出连接。
    2.  **维护暖池**: 启动后台任务，根据 `warmPool` 配置（`minSize`, `maxSize`, `autoScale`）和连接借出速率，决定是否需要填充连接池。
    3.  **填充连接**: 当需要填充时，调用 `ISelector` 选择一个最佳后端，尝试建立新连接。
    4.  **被动探测**: 将连接建立的结果（成功/失败、耗时）报告给 `IScoringSystem` 进行评分更新。
    5.  **后端同步**: 启动后台任务，定期调用 `IBackendProvider` 刷新后端列表，并将变更同步给 `IScoringSystem` 和 `IHealthProber`。

#### 4.3. IBackendProvider (策略模式)
*   **接口**: `GetBackends() ([]string, error)`
*   **实现**:
    *   `FileBackendProvider`: 从本地文件读取列表。
    *   `HttpBackendProvider`: 从HTTP端点获取列表。
    *   `DnsSrvBackendProvider`: 查询DNS SRV记录。
*   **通用逻辑**: 所有实现都支持对无端口的地址附加 `defaultPort`。

#### 4.4. IWarmPool
*   **接口**: `Get(timeout)`, `Add(conn)`, `Size()`
*   **核心逻辑**:
    1.  **单向流动**: 连接只能被添加 (`Add`) 和借出 (`Get`)。借出的连接在使用完毕后由 `ProxyServer` 直接关闭，**不归还**。
    2.  **借出时检查**: 可配置 (`checkHealthOnGet`) 在借出连接时进行一次快速健康检查，确保返回的连接立即可用。
    3.  **自动伸缩**: `ConnectionManager` 根据 `autoScale` 配置动态调整暖池的目标大小。

#### 4.5. IHealthProber
*   **职责**: 对后端服务进行主动的、持续的健康检查。
*   **初始探测 (`initialProbe`)**:
    *   在系统启动或后端列表更新时触发。
    *   以较高的并发度 (`maxConcurrency`) 对所有后端进行一次性探测，以快速建立初始状态。
*   **主动探测 (`activeProbe`)**:
    *   支持两种策略 (`strategy`):
        *   **`burst` (爆发模式)**: 按 `interval` 周期性地对所有后端发起并发探测。简单直接，但会产生周期性压力。
        *   **`continuous_batch` (连续分批模式)**: **推荐**。以高频率 (`batchInterval`) 对一小批 (`batchSize`) 后端进行探测，使用游标循环遍历整个列表。此模式将探测压力均匀地分布在时间轴上，对系统更友好。
*   **并发控制**: 所有探测活动都受 `maxConcurrency` 限制，防止探测风暴。
*   **结果上报**: 探测结果（成功/失败、延迟）上报给 `IScoringSystem`。

#### 4.6. IScoringSystem
*   **职责**: 维护所有后端的状态和质量评分。
*   **数据结构**: 内部维护一个 `map[string]*BackendInfo`，其中 `BackendInfo` 包含：
    *   IP地址
    *   健康分数 (Score)
    *   延迟滑动平均值
    *   成功率滑动窗口
    *   **熔断器对象 (CircuitBreaker)**
*   **评分逻辑**: 根据 `scoring.weights` 配置，结合探测结果和连接建立结果，动态更新分数。失败会触发 `failurePenalty`。
*   **熔断器 (Circuit Breaker)**:
    *   **状态**: `CLOSED`, `OPEN`, `HALF-OPEN`。
    *   **状态转换**:
        *   `CLOSED` -> `OPEN`: 当连续失败次数达到 `failureThreshold`。
        *   `OPEN` -> `HALF-OPEN`: `OPEN` 状态持续 `resetTimeout` 时间后。
        *   `HALF-OPEN` -> `CLOSED`: 在 `HALF-OPEN` 状态下，连续成功次数达到 `halfOpenSuccesses`。
        *   `HALF-OPEN` -> `OPEN`: 在 `HALF-OPEN` 状态下，一旦有失败发生。
    *   **作用**: `OPEN` 状态的后端会被 `ISelector` 排除，实现快速失败和流量切换。

#### 4.7. ISelector
*   **职责**: 根据评分和策略，智能地选择一个后端用于建立新连接。
*   **两阶段选择 (Two-Stage Selection)**:
    1.  **过滤阶段 (Candidate Filtering)**:
        *   从 `IScoringSystem` 获取所有健康（未熔断）且已排序的后端。
        *   根据 `candidateFilter` 配置 (`topN` 或 `topPercent`)，筛选出一个最优的候选人子集。
    2.  **选择阶段 (Final Selection)**:
        *   在候选人子集上应用 `strategy` 配置的负载均衡策略。
*   **支持的策略 (`strategy`)**:
    *   `p2c`: Power of Two Choices，高效的负载均衡策略。
    *   `weighted_random`: 加权随机，分数越高被选中概率越大。
    *   `ip_hash`: 源IP哈希，用于实现会话保持。
    *   `least_load`: 选择近期负载最低的后端。
    *   `highest_score`: 总是选择分数最高的。

### 5. 关键工作流

#### 5.1. 处理一个客户端连接
1.  客户端连接到 `ProxyServer` 的 `listenAddr`。
2.  `ProxyServer` 接受连接，并为其启动一个处理协程。
3.  处理协程调用 `ConnectionManager.GetBackendConnection()`。
4.  `ConnectionManager` 向 `IWarmPool` 请求一个连接。
5.  `IWarmPool` 弹出一个预建连接，（可选）检查其健康，然后返回给 `ConnectionManager`。
6.  `ProxyServer` 收到后端连接后，开始在客户端和后端之间双向转发数据。
7.  任一端断开连接后，转发结束，`ProxyServer` 关闭两个连接。

#### 5.2. 连续分批健康探测
1.  `HealthProber` 的高频定时器触发。
2.  `HealthProber` 从 `IScoringSystem` 获取当前后端列表，并根据内部游标确定本批次要探测的 `batchSize` 个后端。
3.  `HealthProber` 并发（受 `maxConcurrency` 限制）地对这批后端执行TCP拨测或其他类型的探测。
4.  对于每个探测结果，调用 `IScoringSystem.UpdateScore()` 更新对应后端的评分和熔断器状态。
5.  `HealthProber` 更新内部游标，为下一批探测做准备。

### 6. 配置规范

系统的所有行为由一份YAML文件驱动。以下是包含所有功能的完整配置模板。

```yaml
# ===================================================================
# TCP Forwarder - 全局配置
# ===================================================================

# 服务端核心配置
server:
  # 代理服务监听的地址和端口
  listenAddr: "0.0.0.0:8888"
  # 优雅停机等待超时时间。收到SIGTERM/SIGINT信号后，等待现有连接处理完毕的最长时间。
  gracefulShutdownTimeout: "30s"

# 日志配置
logging:
  # 日志级别: "debug", "info", "warn", "error"
  # debug: 最详细，用于开发调试。
  # info: 正常运行信息，生产环境推荐。
  # warn: 警告，可能存在的问题。
  # error: 错误，需要关注处理。
  level: "info"
  
  # 日志输出格式: "text" 或 "json"
  # text: 人类可读格式。
  # json: 机器可读格式，便于集成到ELK、Splunk等日志分析系统。
  format: "json"
  
  # 日志输出位置: "stdout" 或 文件路径
  # stdout: 直接输出到控制台。
  # stderr: 输出到错误流
  # e.g., "/var/log/tcp_forwarder.log": 输出到指定文件，会自动进行日志轮转。
  output: "stdout"

# ===================================================================
# 后端服务发现 (Backend Provider)
# ===================================================================
backendSource:
  # 类型: "file", "http", "dns_srv"
  # file: 从本地文件读取后端列表。
  # http: 通过HTTP API动态获取后端列表。
  # dns_srv: 通过查询DNS SRV记录发现后端。
  type: "file"

  # 【当 type=file 时使用】
  # 文件路径，每行一个后端地址，例如 "10.0.1.1:3306" 或 "10.0.1.1"。
  path: "/etc/tcp_forwarder/backends.list"

  # 【当 type=http 时使用】
  # 提供后端列表的HTTP API端点。
  url: "http://config-server/api/backends"
  # 预期返回的文本格式，同文件
  
  # 【当 type=dns_srv 时使用】
  # 要查询的DNS SRV记录，格式为 "_service._proto.name"。
  # 例如: "_mysql._tcp.db.example.com"
  srvRecord: "_mysql._tcp.db.example.com"
  
  # 当后端地址（来自文件或HTTP）没有指定端口时，使用的默认端口。
  # SRV记录会自带端口，此配置对其无效。
  defaultPort: 3306

  # 是否监视后端变动。如果为 true，当后端内容发生变化时，应用会自动重载IP列表。(暂未实现)
  watch: false

  # 刷新后端列表的间隔。适用于所有类型，用于动态更新。
  refreshInterval: "60s"

# ===================================================================
# 暖连接池 (Warm Pool)
# ===================================================================
warmPool:
  # 池中保持的最小连接数。即使在空闲时，池也会努力填充到这个数量。
  minSize: 10
  # 池中允许存在的最大连接数。防止无限创建连接消耗资源。
  maxSize: 100
  
  # 借出连接时，是否对其进行一次快速健康检查。
  # 如果为true，会增加一点点延迟，但能确保拿到的连接绝对可用。
  # 如果为false，性能更高，但不保证连接的可用。
  checkHealthOnGet: true

  # 连接池填充检查间隔。每隔这么久，ConnectionManager会检查是否需要创建新连接。
  refillInterval: "1s"
  # 从池中获取一个健康连接的总超时时间。
  getTimeout: "500ms"

  # 自动伸缩配置
  autoScale:
    # 是否启用自动伸缩。如果为 false，连接池将始终以 minSize 为目标。
    enabled: true

    # --- 核心决策参数 ---
    # 伸缩决策的评估间隔。每隔这么久，系统会重新计算一次目标池大小。
    # 建议值: "5s" - "30s"
    evaluationInterval: "10s"
    
    # --- 扩容策略 (Scale-Up) ---
    # 用于计算“平均借出速率”的时间窗口。
    # 较短的窗口对流量突增反应更快，但可能不稳定。
    # 较长的窗口更平滑，但反应较慢。
    # 建议值: "60s" - "300s"
    metricsWindow: "60s"
    
    # 动态目标连接数的计算因子。
    # targetSize = minSize + (metricsWindow内的平均每秒借出速率 * borrowRateFactor)
    # 因子越大，池对流量的反应越激进，会储备更多的连接。
    # 建议值: 0.5 - 5.0
    borrowRateFactor: 1.5

    # --- 缩容策略 (Scale-Down) ---
    # 当流量下降时，如何降低池的目标大小。
    # 池中的连接不会被主动销毁，而是通过降低填充目标，让它们在使用后自然关闭而不被补充。
    # 经过多长时间的持续低利用率后，开始进行缩容决策。
    # “低利用率”定义为：池中连接数 > (minSize + 当前速率所需的连接数)。
    # 建议值: "300s" (5分钟)
    idleTimeout: "300s"

    # --- 稳定性参数 ---
    # 在一次扩容操作后，需要等待多久才能进行下一次（无论是扩容还是缩容）决策。
    # 这可以防止因扩容效果的延迟反馈而导致的连续、不必要的扩容（“抖动”）。
    # 建议值: "60s" - "180s"
    stabilizationWindow: "120s"


# ===================================================================
# 健康探测 (Health Prober)
# ===================================================================
healthProbe:
  # 主动探测的间隔。对所有后端进行一次健康检查的周期。
  activeInterval: "5s"
  # 每次探测的超时时间。
  timeout: "2s"
  
  # 探测类型: "tcp_dial", "http_get", "icmp"
  type: "tcp_dial"

  # 【当 type=http_get 时使用】
  http_options:
    path: "/health"
    expectedStatusCode: 200

  # --- 初始探测配置 ---
  # 用于系统启动或后端列表发生变化时，对所有后端进行的一次性全量探测。
  # 这种场景通常适合使用 "burst" 模式快速获取所有节点状态。
  initialProbe:
    maxConcurrency: 20

  # --- 主动探测配置 ---
  # 用于在后台周期性、持续性地检查所有后端健康状况。
  activeProbe:
    # 探测策略: "burst" 或 "continuous_batch"
    # burst: 周期性地对所有后端发起并发探测，会产生瞬时压力。
    # continuous_batch: (推荐) 持续、平滑地分批次探测，将压力均匀分布。
    strategy: "continuous_batch"

    # 【当 strategy = "burst" 时使用】
    burst_options:
      # 探测所有后端的总间隔时间。
      interval: "5s"
      # 探测时的最大并发数。
      maxConcurrency: 10
    
    # 【当 strategy = "continuous_batch" 时使用】
    continuous_batch_options:
      # 每一批次探测的后端数量。
      # 建议值: 2 ~ 10。
      batchSize: 4
      # 探测完一批后，到探测下一批之间的间隔时间。
      # 这个值决定了探测的平滑度。值越小，扫描越快，但持续的背景压力略高。
      # 建议值: "100ms" ~ "500ms"。
      batchInterval: "200ms"
      # 任何时刻，允许同时在途 (in-flight) 的最大探测请求数。
      # 这是一个安全阀，防止 batchSize 设置过大时产生过多并发。
      # 实际并发数将是 min(batchSize, maxConcurrency)。
      # 建议值: 5 ~ 15。
      maxConcurrency: 10

# ===================================================================
# 评分系统 (Scoring System)
# ===================================================================
scoring:
  # 评分公式各项的权重。总和不必为1。
  weights:
    # 延迟的权重。分数与延迟成反比 (score_part = 1/latency)。
    latency: 0.6
    # 连接成功率的权重。
    successRate: 0.4
  # 一次失败探测/连接对分数的惩罚值。可以快速降低问题节点的分数。
  failurePenalty: 10.0

# ===================================================================
# 选择器 (Selector) - 【增强版】
# ===================================================================
selector:
  # --- 阶段1: 候选人过滤 (Candidate Filtering) ---
  # 在应用选择策略之前，先将后端范围缩小到一个更优的子集。
  # 这有助于集中负载到表现最好的节点上，并忽略表现差的节点。
  candidateFilter:
    # 只从分数最高的 N 个后端中进行选择。
    # 设置为 0 或不设置表示禁用。
    topN: 10
    
    # 只从分数最高的百分之 X 的后端中进行选择。
    # 例如 20 表示从前 20% 的后端中选择。
    # 如果 topN 和 topPercent 同时设置，topN 优先。
    # 设置为 0 或不设置表示禁用。
    topPercent: 0

  # --- 阶段2: 最终选择策略 (Final Selection Strategy) ---
  # 在 "候选人池" 上应用此策略。
  # 可选: "p2c", "weighted_random", "least_load", "highest_score"
  strategy: "p2c"

  # 【当 strategy=p2c (Power of Two Choices) 时】
  # 优点: 极其高效的负载均衡，随机性和性能导向的完美结合。推荐！

  # 【当 strategy=weighted_random (加权随机) 时】
  # 优点: 根据分数分配流量，性能越好承担越多，平滑过渡。

  # 【当 strategy=least_load (最少负载) 时】
  # 优点: 尝试将新连接分配给当前最空闲的服务器。
  # "负载"被定义为"最近N秒内的连接借出次数"。
  least_load_options:
    # 定义“近期”的时间窗口。
    timeWindow: "60s"
    
# ===================================================================
# 熔断器 (Circuit Breaker)
# ===================================================================
circuitBreaker:
  # 在进入 "Open" 状态前，允许的连续失败次数。
  failureThreshold: 5
  
  # 熔断器 "Open" 状态持续时间，之后会自动转为 "Half-Open" 状态。
  resetTimeout: "30s"
  
  # 在 "Half-Open" 状态下，需要多少次连续的成功探测/连接才能将熔断器关闭 ("Closed")。
  # 这可以防止因一次偶然的成功就立即恢复流量。
  halfOpenSuccesses: 2
```

---

## 7. Rust 实现落地方案

### 7.1 模块划分 (crate 内部目录结构)

```
src/
  main.rs                # 入口：加载配置、初始化全局组件、启动 proxy
  config.rs              # 配置结构体 & 解析 & 日志初始化
  types.rs               # 通用类型定义（别名、错误、常量）
  backend/
    mod.rs               # trait IBackendProvider + 枚举装配
    file.rs              # 文件实现
    http.rs              # HTTP 实现
    dns_srv.rs           # DNS SRV 实现
  health/
    mod.rs               # IHealthProber 接口 & 调度器
    prober.rs            # 具体探测执行逻辑 (tcp/http/icmp)
  scoring/
    mod.rs               # IScoringSystem trait
    window.rs            # 滑动窗口 (环形缓冲 / 时间桶)
    circuit_breaker.rs   # 熔断器实现
    scorer.rs            # 分数计算核心
  selector/
    mod.rs               # ISelector trait
    strategies.rs        # p2c / weighted_random / least_load / highest_score / ip_hash
  pool/
    mod.rs               # IWarmPool trait
    warm_pool.rs         # 单向借出池实现
  conn/
    manager.rs           # ConnectionManager
    dialer.rs            # 建立新TCP连接的异步封装
  proxy/
    server.rs            # ProxyServer：accept + 双向转发
  util/
    time.rs              # 时间/节流/指数平滑工具
    rand.rs              # 统一随机工具 (SmallRng)
    metrics.rs           # (预留) 指标埋点接口
```

### 7.2 并发与运行时模型

* Tokio 多线程 runtime (`#[tokio::main(flavor = "multi_thread")]`).
* 后台长期任务：
  * 后端刷新任务 (interval)
  * 健康探测调度任务 (burst / continuous batch)
  * 连接池 refill 任务
  * 自动伸缩评估任务
  * 熔断状态巡检 (可与评分合并，事件驱动即可减少轮询)
* 短期任务：单个探测 / 单个新连接拨测 / 单个客户端转发协程。

### 7.3 关键 trait 定义草案

```rust
#[async_trait::async_trait]
pub trait BackendProvider: Send + Sync {
    async fn fetch(&self) -> anyhow::Result<Vec<String>>; // 统一返回 host:port
}

#[async_trait::async_trait]
pub trait HealthProber: Send + Sync {
    async fn initial_probe(&self, backends: &[BackendId]);
    fn start_active(&self); // 内部自行 spawn 循环任务
}

pub trait ScoringSystem: Send + Sync {
    fn all_backends_sorted(&self) -> Vec<BackendSnapshot>; // 已过滤 OPEN 熔断
    fn report_probe(&self, id: BackendId, r: ProbeResult);
    fn report_connection(&self, id: BackendId, r: ConnResult);
}

pub trait Selector: Send + Sync {
    fn select(&self, source_ip: Option<IpAddr>) -> Option<BackendSnapshot>;
}

#[async_trait::async_trait]
pub trait WarmPool: Send + Sync {
    async fn get(&self, timeout: Duration) -> anyhow::Result<PooledConn>;
    fn push_new(&self, conn: BackendConn);
    fn size(&self) -> usize;
}
```

### 7.4 数据结构与性能考量

| 需求 | 结构 | 说明 |
|------|------|------|
| 高并发读 (选择器读取状态) | `Arc<DashMap<BackendId, BackendState>>` | DashMap 支持多 shard 并发读写 |
| 有序访问 (按分数排序) | 取快照后本地排序 | 避免全局锁；排序结果短期有效即可 |
| 滑动窗口成功率 | 时间桶 (VecDeque) + 原子累积 | 固定桶数量，O(1) 更新 |
| 延迟统计 | 指数移动平均 (EMA) | 避免存储大量样本 |
| 连接池 | `tokio::sync::mpsc` + `parking_lot::Mutex` | 出队非阻塞+async 等待 refill |

### 7.5 连接池单向借出实现思路

* 内部保持一个 `Vec<BackendConn>` (或分 backend 分桶)；借出使用 pop。 
* 当 size < minSize 或 (autoScale 目标 > 当前 size 且 refill 通道空闲) 时，refill 任务尝试创建新连接。
* 借出失败 (池空) 时：
  1. 快速路径：立即触发一次 opportunistic refill。
  2. 等待通知（`tokio::sync::Notify` + 超时）。
  3. 超时则上抛错误。
* 借出的连接不归还；连接关闭由 Proxy 转发结束后执行。

### 7.6 两阶段选择算法示例实现草案

```rust
pub fn filter_candidates(all: &[BackendSnapshot], cfg: &CandidateFilterCfg) -> &[BackendSnapshot] {
    if all.is_empty() { return &all[0..0]; }
    if let Some(top_n) = cfg.top_n.filter(|&n| n > 0) {
        let n = n.min(all.len());
        &all[..n]
    } else if let Some(percent) = cfg.top_percent.filter(|&p| p > 0) {
        let mut n = (all.len() * percent as usize) / 100;
        if n == 0 { n = 1; }
        &all[..n]
    } else { all }
}

pub fn p2c(cands: &[BackendSnapshot], rng: &mut impl Rng) -> Option<&BackendSnapshot> {
    if cands.is_empty() { return None; }
    if cands.len() == 1 { return Some(&cands[0]); }
    let a = rng.gen_range(0..cands.len());
    let mut b = rng.gen_range(0..cands.len());
    if a == b { b = (b + 1) % cands.len(); }
    Some(if cands[a].score >= cands[b].score { &cands[a] } else { &cands[b] })
}
```

### 7.7 熔断器实现要点

* 使用 `AtomicU32` 存储连续失败计数；状态使用 `AtomicU8` + enum 映射。
* 进入 OPEN 时记录 `opened_at: Instant`。
* 读路径：只读原子；写路径：compare_exchange 降低竞态。
* HALF-OPEN 仅允许“试探”窗口内的少量请求 (可选扩展：配额 N)。

### 7.8 健康探测调度 (continuous_batch)

* 维护 `cursor: AtomicUsize`。
* 每次 tick：
  1. 读取当前 backend 快照长度 L。
  2. 起始 = cursor.fetch_add(batch_size) % L。
  3. 选取 `[start .. start+batch_size)` 取模环绕。
* 为避免长拨测阻塞：对每个探测 spawn task，使用 `Semaphore` 限制并发。

### 7.9 配置解析与热更新

* 启动时一次性加载 YAML -> `AppConfig`。
* 后端列表刷新独立，与整体配置热更新解耦；若后续需要整体配置热更新，可：
  1. 监听 SIGHUP。
  2. 重新解析；对差异项应用动态更新（例如日志级别、池目标、探测间隔）。
  3. 不可动态修改的参数（如某些结构初始容量）记录 warning。

### 7.10 错误分层

| 层级 | 类型 | 处理策略 |
|------|------|----------|
| I/O 拨测 | 超时 / 连接拒绝 | 计为失败，报告 scoring & circuit breaker |
| 配置错误 | 缺字段 / 解析失败 | 启动即退出 (fatal) |
| 后端源 | 暂时失败 | 保留旧列表 + warn |
| 运行期内部 bug | 不可恢复 panic | 进程崩溃并交由上层编排器重启 |

### 7.11 可观测性

* 日志：tracing + EnvFilter；支持 text/json。
* 结构化字段：`backend=..., event=probe, latency_ms=..., success=true`。
* 指标 (后续扩展)：
  * `probe_latency_histogram{backend=}`
  * `backend_score_gauge{backend=}`
  * `circuit_state{backend=}` (0/1/2)
  * `warm_pool_size`
  * `connections_active`

### 7.12 后续扩展路线 (Roadmap)

1. v1: TCP 转发 + file backend + tcp_dial 探测 + p2c + 基础熔断 + 单向连接池。
2. v1.1: HTTP backend provider + weighted_random + metrics (Prometheus)。
3. v1.2: DNS SRV + least_load + ip_hash。
4. v2: 配置热更新 + 优雅停机 + TLS 前端/后端支持。
5. v2.1: 多租户 namespace 隔离 + ACL。

### 7.13 最小可运行版本 (MVP) 列表

| 组件 | 必要 | 说明 |
|------|------|------|
| 配置加载 | 是 | server.listenAddr + backend.file + warmPool 基础 |
| ProxyServer | 是 | accept + 双向复制 (tokio::io::copy_bidirectional) |
| BackendProvider(file) | 是 | 从文件读取并解析地址 |
| ScoringSystem | 是 | 基础分数 + 熔断 (仅 CLOSED/OPEN) |
| Selector(p2c) | 是 | 来自排序列表随机二选一 |
| WarmPool | 是 | minSize 填充 + get 超时 |
| HealthProber | 可选 | v1 可先用“被动”连接失败反馈 |

---

（以上 Rust 实现章节为在原有架构基础上的具体落地设计，便于后续按阶段逐步实现。）
