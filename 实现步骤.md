### 项目目标
构建一个高性能、高可用的 Rust TCP 转发器，它能：
1.  从配置源加载候选 IP 列表。
2.  通过智能评分模型（基于延迟、成功率等）动态评估 IP 质量。
3.  周期性地选择最优的一组 IP 作为活跃目标。
4.  为活跃 IP 维护一个“暖连接池”，以实现新连接的零延迟转发。
5.  在 IP 间切换时，对用户体验影响降到最低（无缝切换）。
6.  具备可观测性（日志、Prometheus 指标）。

### 核心技术栈
*   **异步运行时**: `tokio`
*   **配置管理**: `serde` + `serde_yaml` + `config-rs`
*   **日志**: `tracing` + `tracing-subscriber`
*   **并发数据结构**: `dashmap` (用于评分表), `tokio::sync` (MPSC Channels, RwLock)
*   **指标**: `metrics` crate + `axum` (用于暴露端点)
*   **时间处理**: `humantime-serde`

---

## 实现计划：分步走

### 阶段 0：项目奠基 (Project Foundation)
**目标**: 搭建项目骨架，配置加载和日志系统正常工作。
**产出**: 一个可以运行但什么都不做的程序，但已具备基础框架。

1.  **初始化项目**:
    *   运行 `cargo new tcp-forwarder`。
    *   在 `Cargo.toml` 中添加核心依赖: `tokio`, `serde`, `serde_yaml`, `config`, `tracing`, `tracing-subscriber`, `humantime-serde`, `anyhow`.

2.  **定义配置结构**:
    *   创建 `src/config.rs` 文件。
    *   根据我们设计的 `config.yaml`，使用 `serde::Deserialize` 宏定义出完整的 Rust `struct` 结构，包括所有嵌套模块。
    *   使用 `#[serde(with = "humantime_serde")]` 为所有 `Duration` 类型的字段添加注解。

3.  **实现配置加载**:
    *   在 `src/main.rs` 中，编写一个函数用于读取 `config.yaml` 文件并将其反序列化为配置 `struct`。使用 `config-rs` crate 会非常方便。

4.  **设置日志系统**:
    *   在 `main` 函数的开头，根据配置文件中的 `logging` 部分初始化 `tracing-subscriber`。实现对日志级别、格式（text/json）和输出目标（stdout/file）的支持。

**验证**: 运行程序，它应该能成功加载配置文件并打印出一条日志，表明服务已启动。

---

### 阶段 1：核心转发功能 (MVP)
**目标**: 实现一个最简单的 TCP 代理，将所有连接转发到一个固定的、硬编码的远程地址。
**产出**: 一个基本的、能工作的 TCP 代理。

1.  **创建监听器**:
    *   在 `main` 函数中，使用 `tokio::net::TcpListener::bind` 绑定到配置中 `server.listen_addr`。

2.  **接受并处理连接**:
    *   在一个 `loop` 中调用 `listener.accept().await`。
    *   每当接受一个新连接 (`client_socket`)，使用 `tokio::spawn` 将其处理逻辑放到一个新的异步任务中，以避免阻塞主循环。

3.  **实现转发逻辑**:
    *   在新的异步任务中，硬编码一个目标地址（例如 `1.1.1.1:443`）。
    *   使用 `tokio::net::TcpStream::connect` 连接到该目标地址，得到 `remote_socket`。
    *   使用 `tokio::io::copy_bidirectional(&mut client_socket, &mut remote_socket).await` 来双向拷贝数据。
    *   添加基本的错误处理和日志记录（例如，连接远程失败、转发中断等）。

**验证**: 运行程序。使用 `curl --proxy 127.0.0.1:1234 https://www.google.com` 或 `netcat` 工具进行测试，流量应该能被成功转发。

---

### 阶段 2：智能评分系统
**目标**: 实现评分计算和数据探测的核心逻辑。
**产出**: 一个后台任务，能周期性地探测 IP 并更新其分数。

1.  **创建 `scorer` 和 `probing` 模块**:
    *   创建 `src/scorer.rs` 和 `src/probing.rs`。

2.  **定义数据结构**:
    *   在 `scorer.rs` 中，定义 `ScoreData` 结构，包含延迟、抖动、成功率的 EWMA (指数加权移动平均) 值、失败惩罚、历史奖励和总分。可以为 EWMA 创建一个简单的辅助 `struct`。
    *   定义一个全局共享的评分板 `ScoreBoard`，类型为 `pub type ScoreBoard = Arc<DashMap<IpAddr, ScoreData>>;`。

3.  **实现 `Probing` 任务**:
    *   在 `probing.rs` 中，创建一个 `probing_task` 异步函数。
    *   该函数包含一个基于 `probing.interval` 的 `tokio::time::interval` 循环。
    *   在循环中，从 `ScoreBoard` 中挑选 `probe_candidate_count` 个最需要更新的 IP（例如，最久未被探测的）。
    *   对每个选中的 IP，使用 `tokio::time::timeout` 包裹 `TcpStream::connect` 来进行探测。
    *   根据连接结果（成功/失败、耗时），更新 `ScoreBoard` 中对应 IP 的 `ScoreData`。这一步是评分模型的核心，需要仔细实现 EWMA 和惩罚/奖励逻辑。

4.  **集成到 `main`**:
    *   在 `main` 函数中，初始化 `ScoreBoard`。
    *   从配置文件加载 IP 列表，并用初始数据填充 `ScoreBoard`。
    *   `tokio::spawn` 启动 `probing_task`。

**验证**: 运行程序，观察日志输出。应该能看到探测任务周期性运行，并打印出 IP 分数的变化情况。

---

### 阶段 3：选择器与动态切换
**目标**: 实现周期性的评估和最优 IP 选择。
**产出**: 系统的“大脑”开始工作，能够根据评分动态选择出活跃 IP 列表。

1.  **创建 `selector` 模块**:
    *   创建 `src/selector.rs`。

2.  **定义共享状态**:
    *   创建一个全局共享的“活跃集”：`pub type ActiveRemotes = Arc<RwLock<Vec<IpAddr>>>;`。

3.  **实现 `Selector` 任务**:
    *   在 `selector.rs` 中，创建 `selector_task` 异步函数。
    *   该函数包含一个基于 `selector.evaluation_interval` 的 `tokio::time::interval` 循环。
    *   在循环中：
        a.  从 `ScoreBoard` 读取所有 IP 的最新分数。
        b.  对 IP 按分数进行排序。
        c.  应用 `selector` 配置中的所有规则：`min_score_threshold`, `active_set_size`, 以及 `switching` 防抖策略（这是关键的复杂逻辑）。
        d.  计算出新一轮的“目标活跃 IP 列表”。
        e.  获取 `ActiveRemotes` 的写锁，并更新列表。

4.  **修改转发逻辑**:
    *   修改阶段 1 的转发任务。不再连接硬编码的地址，而是：
        a.  获取 `ActiveRemotes` 的读锁。
        b.  从列表中选择一个 IP（例如，随机或轮询选择）。
        c.  连接到这个被选中的 IP。

**验证**: 运行程序，通过日志观察 `selector_task` 是否周期性地选出并更新了活跃 IP 列表。同时，转发功能现在应该会动态地将流量发往这些高分 IP。

---

### 阶段 4：实现暖连接池
**目标**: 引入暖池，消除新连接的建立延迟。
**产出**: 一个高性能、低延迟的转发器。

1.  **创建 `pools` 模块**:
    *   创建 `src/pools.rs`。

2.  **实现 `PoolManager`**:
    *   这是项目的核心之一。创建一个 `PoolManager` 结构体，它负责管理所有活跃 IP 的连接池。
    *   内部数据结构可以是 `DashMap<IpAddr, MpscSender<TcpStream>>`。
    *   创建一个 `pool_manager_task`，它会监听 `ActiveRemotes` 的变化。
    *   当 `ActiveRemotes` 列表更新时：
        *   **新增 IP**: 为新 IP 创建一个新的 MPSC channel，并 `spawn` 一个新的 `filler_task`。
        *   **移除 IP**: 停止该 IP 的 `filler_task`，并开始排空（drain）其连接池。
    *   `filler_task(ip, sender)` 的职责是不断地创建到 `ip` 的连接，并将成功的 `TcpStream` 发送到 `sender`，以维持池中的连接数。它也需要将连接结果报告给 `ScoreBoard`。

3.  **最终修改转发逻辑**:
    *   修改 `main` 函数中的连接处理任务。现在它不再是自己去 `connect`，而是：
        a.  从 `ActiveRemotes` 中通过负载均衡算法（如 `least_connections`）选择一个目标 IP。
        b.  从 `PoolManager` 中获取该 IP 对应的 MPSC Receiver。
        c.  尝试从 Receiver `recv()` 一个预先建立好的 `TcpStream`。
        d.  **回退机制**: 如果池为空（`try_recv` 失败），则立即为当前请求建立一个新连接作为降级方案，保证服务可用。

**验证**: 使用压测工具（如 `bombardier`）进行测试。新连接的建立时间应该极短，系统吞吐量有显著提升。观察日志，确认连接池的创建、填充和排空逻辑正常。

---

### 阶段 5：生产环境强化
**目标**: 增加可观测性，并使服务更健壮。
**产出**: 一个准生产级别的应用。

1.  **集成 Prometheus 指标**:
    *   在 `Cargo.toml` 中添加 `metrics` 和 `axum`。
    *   在代码的关键路径上（如新连接、连接池命中/未命中、IP 分数、活跃连接数等）插入 `metrics::counter!`, `metrics::gauge!`, `metrics::histogram!` 宏。
    *   `spawn` 一个 `axum` 服务器，用于在 `/metrics` 路径上暴露指标。

2.  **实现动态连接池伸缩**:
    *   如果配置为 `dynamic`，`filler_task` 的逻辑需要更复杂。它需要监控近期的连接请求速率，并根据 `scaling` 配置动态调整目标池大小。

3.  **实现优雅停机 (Graceful Shutdown)**:
    *   在 `main` 函数中，监听 `Ctrl+C` 信号 (`tokio::signal::ctrl_c`)。
    *   收到信号后，通知所有任务开始关闭：停止监听新连接、排空连接池、等待所有正在进行的转发任务结束。

**验证**: 启动服务，访问 `http://127.0.0.1:9099/metrics` 查看指标。使用 Grafana 创建仪表盘。尝试按 `Ctrl+C`，服务应该能优雅退出，而不是立即崩溃。

---

### 阶段 6：收尾工作
**目标**: 完善项目，使其易于使用和部署。

1.  **编写测试**:
    *   为 `scorer` 中的评分逻辑编写单元测试。
    *   为核心的转发和连接池逻辑编写集成测试。
2.  **完善文档**:
    *   编写一个详细的 `README.md`，解释项目功能、如何配置和运行。
3.  **打包**:
    *   编写一个 `Dockerfile`，用于将应用容器化部署。

通过以上六个阶段，你可以系统性地、稳健地完成这个复杂而有趣的项目。祝你编码愉快！